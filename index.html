<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Enhancing Intent Understanding for Ambiguous prompt: A Human-Machine Co-Adaption Strategy</title>
  <link rel="icon" type="image/png" href="static/images/favicon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Enhancing Intent Understanding for Ambiguous prompt: A Human-Machine Co-Adaption Strategy
</h1>

                  <div class="column has-text-centered">
                                      <div class="publication-links">
                                           <!-- Arxiv PDF link -->
                                        <span class="link-block">
                                          <a href="https://arxiv.org/pdf/2501.15167" target="_blank"
                                          class="external-link button is-normal is-rounded is-dark">
                                          <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                          </span>
                                          <span>Paper</span>
                                        </a>
                                      </span>
                  
                  
                                    <span class="link-block">
                                      <a href="https://huggingface.co/datasets/naseele/Image_prompt/tree/main" target="_blank"
                                      class="external-link button is-normal is-rounded is-dark">
                                      <span class="icon">
                                        <img src="https://huggingface.co/front/assets/huggingface_logo.svg"
                       alt="Hugging Face"
                       style="width:2rem; height:auto;">
                                      </span>
                                      <span>Release</span>
                                    </a>
                                  </span>
                  
                  
                  
                                  <!-- ArXiv abstract Link -->
                                  <span class="link-block">
                                    <a href="https://arxiv.org/abs/2501.15167" target="_blank"
                                    class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                      <i class="ai ai-arxiv"></i>
                                    </span>
                                    <span>arXiv</span>
                                  </a>
                                </span>


            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
Today's image generation systems are capable of producing realistic and high-quality images. However, user prompts often contain ambiguities, making it difficult for these systems to interpret users' actual intentions. Consequently, many users must modify their prompts several times to ensure the generated images meet their expectations. While some methods focus on enhancing prompts to make the generated images fit user needs, the model is still hard to understand users' real needs, especially for non-expert users. In this research, we aim to enhance the visual parameter-tuning process, making the model user-friendly for individuals without specialized knowledge and better understand user needs. We propose a human-machine co-adaption strategy using mutual information between the user's prompts and the pictures under modification as the optimizing target to make the system better adapt to user needs. We find that an improved model can reduce the necessity for multiple rounds of adjustments. We also collect multi-round dialogue datasets with prompts and images pairs and user intent. Various experiments demonstrate the effectiveness of the proposed method in our proposed dataset. Our annotation tools and several examples of our dataset are available at https://zenodo.org/records/14876029 for easier review. We will make open source our full dataset and code.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Method</h2>
      <div class="box" style="max-width:1000px; margin:0 auto;">
        <img 
          src="static/images/framework.jpg" 
          alt="SCORE framework overview" 
          style="width:100%; height:auto; display:block; margin:0 auto;">
        <h2 class="subtitle" 
            style="margin-top:1rem; text-align:justify;">
            Overview of the overall framework: (a) The LLM refines the user's prompt through Word Swap, Adding a Phrase, or Attention Re-weighting, using Proximal Policy Optimization Methods (PPO) until satisfaction; (b) Effects of these operations on the image are demonstrated; (c) PPO's internal loop computes rewards based on CLIP feedback to meet the threshold or limit.
        </h2>
      </div>
    </div>
  </div>
</section>





<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{he2025enhancingintentunderstandingambiguous,
      title={Enhancing Intent Understanding for Ambiguous prompt: A Human-Machine Co-Adaption Strategy}, 
      author={Yangfan He and Jianhui Wang and Yijin Wang and Kun Li and Yan Zhong and Xinyuan Song and Li Sun and Jingyuan Lu and Sida Li and Haoyuan Li and Jiayi Su and Jinhua Song and Miao Zhang and Tianyu Shi},
      year={2025},
      eprint={2501.15167},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2501.15167}, 
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
